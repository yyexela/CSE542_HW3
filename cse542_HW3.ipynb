{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49tMkLc06wYc"
      },
      "outputs": [],
      "source": [
        "# Perform git clone as follows\n",
        "\n",
        "!git clone https://github.com/WEIRDLabUW/cse542_sp24_hw2.git\n",
        "!cp -r cse542_sp24_hw2/* .\n",
        "\n",
        "# !NOTE!: Once you are done, copy your implementation of policy gradient, actor critic and\n",
        "# in the notebook here back to the python script\n",
        "# when submiting your code\n",
        "\n",
        "!apt-get install -y \\\n",
        "    libgl1-mesa-dev \\\n",
        "    libgl1-mesa-glx \\\n",
        "    libglew-dev \\\n",
        "    libosmesa6-dev \\\n",
        "    software-properties-common\n",
        "\n",
        "!apt-get install -y patchelf\n",
        "!pip install setuptools==65.5.0 \"wheel<0.40.0\"\n",
        "!pip install gym==0.19.0\n",
        "!pip install gymnasium==0.29.1\n",
        "!pip install gymnasium-robotics[mujoco-py]\n",
        "!pip install gym-notices==0.0.8\n",
        "!pip install matplotlib\n",
        "!pip install mujoco\n",
        "!pip install free-mujoco-py\n",
        "!pip install pybullet\n",
        "import os\n",
        "os.environ['LD_PRELOAD']=':/usr/lib/x86_64-linux-gnu/libGLEW.so'\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from IPython.display import clear_output\n",
        "import argparse\n",
        "import collections\n",
        "import functools\n",
        "import math\n",
        "import time\n",
        "import random\n",
        "from typing import Any, Callable, Dict, Optional, Sequence, List\n",
        "import gym\n",
        "import mujoco_py\n",
        "from gym import utils\n",
        "from gym.envs.mujoco import mujoco_env\n",
        "import torch.nn.functional as F\n",
        "import copy\n",
        "from typing import Tuple, Optional, Union\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import math\n",
        "import copy\n",
        "import numpy as np\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "from torch import distributions as pyd\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "import random\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class ReplayBuffer(object):\n",
        "    \"\"\"Buffer to store environment transitions.\"\"\"\n",
        "\n",
        "    def __init__(self, obs_size, action_size, capacity, device):\n",
        "        self.capacity = capacity\n",
        "        self.device = device\n",
        "\n",
        "        self.obses = np.empty((capacity, obs_size), dtype=np.float32)\n",
        "        self.next_obses = np.empty((capacity, obs_size), dtype=np.float32)\n",
        "        self.actions = np.empty((capacity, action_size), dtype=np.float32)\n",
        "        self.rewards = np.empty((capacity, 1), dtype=np.float32)\n",
        "        self.not_dones = np.empty((capacity, 1), dtype=np.float32)\n",
        "\n",
        "        self.idx = 0\n",
        "        self.last_save = 0\n",
        "        self.full = False\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.capacity if self.full else self.idx\n",
        "\n",
        "    def add(self, obs, action, reward, next_obs, done):\n",
        "        idxs = np.arange(self.idx, self.idx + obs.shape[0]) % self.capacity\n",
        "        self.obses[idxs] = copy.deepcopy(obs)\n",
        "        self.actions[idxs] = copy.deepcopy(action)\n",
        "        self.rewards[idxs] = copy.deepcopy(reward)\n",
        "        self.next_obses[idxs] = copy.deepcopy(next_obs)\n",
        "        self.not_dones[idxs] = 1.0 - copy.deepcopy(done)\n",
        "\n",
        "        self.full = self.full or (self.idx + obs.shape[0] >= self.capacity)\n",
        "        self.idx = (self.idx + obs.shape[0]) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        idxs = np.random.randint(0,\n",
        "                                 self.capacity if self.full else self.idx,\n",
        "                                 size=batch_size)\n",
        "        obses = torch.as_tensor(self.obses[idxs], device=self.device).float()\n",
        "        actions = torch.as_tensor(self.actions[idxs], device=self.device)\n",
        "        rewards = torch.as_tensor(self.rewards[idxs], device=self.device)\n",
        "        next_obses = torch.as_tensor(self.next_obses[idxs],\n",
        "                                     device=self.device).float()\n",
        "        not_dones = torch.as_tensor(self.not_dones[idxs], device=self.device)\n",
        "\n",
        "        return obses, actions, rewards, next_obses, not_dones\n",
        "\n",
        "\n",
        "def set_random_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "class DeterministicDynamicsModel(nn.Module):\n",
        "    def __init__(self, num_inputs, num_outputs, hidden_dim=64, hidden_depth=2):\n",
        "        super(DeterministicDynamicsModel, self).__init__()\n",
        "        self.num_inputs = num_inputs\n",
        "        self.num_outputs = num_outputs\n",
        "        self.trunk = mlp(num_inputs, hidden_dim, num_outputs, hidden_depth)\n",
        "\n",
        "    def forward(self, x):\n",
        "        v = self.trunk(x)\n",
        "        v = v + x[:, :v.shape[1]]\n",
        "        return v\n",
        "\n",
        "def collect_trajs(\n",
        "        env,\n",
        "        agent,\n",
        "        replay_buffer,\n",
        "        device,\n",
        "        episode_length=math.inf,\n",
        "        render=False,\n",
        "):\n",
        "    # Collect the following data\n",
        "    raw_obs = []\n",
        "    raw_next_obs = []\n",
        "    actions = []\n",
        "    rewards = []\n",
        "    dones = []\n",
        "    images = []\n",
        "\n",
        "    path_length = 0\n",
        "\n",
        "    o = env.reset()\n",
        "    if render:\n",
        "        env.render()\n",
        "\n",
        "    while path_length < episode_length:\n",
        "        o_for_agent = o\n",
        "\n",
        "        action, _, _ = agent(torch.Tensor(o_for_agent).unsqueeze(0).to(device))\n",
        "        action= action.cpu().detach().numpy()[0]\n",
        "\n",
        "        # Step the simulation forward\n",
        "        next_o, r, done, env_info = env.step(copy.deepcopy(action))\n",
        "\n",
        "        replay_buffer.add(o,\n",
        "                          action,\n",
        "                          r,\n",
        "                          next_o,\n",
        "                          done)\n",
        "\n",
        "        # Render the environment\n",
        "        if render:\n",
        "            env.render()\n",
        "\n",
        "        raw_obs.append(o)\n",
        "        raw_next_obs.append(next_o)\n",
        "        actions.append(action)\n",
        "        rewards.append(r)\n",
        "        dones.append(done)\n",
        "        path_length += 1\n",
        "        if done:\n",
        "            break\n",
        "        o = next_o\n",
        "\n",
        "    # Prepare the items to be returned\n",
        "    observations = np.array(raw_obs)\n",
        "    next_observations = np.array(raw_next_obs)\n",
        "    actions = np.array(actions)\n",
        "    if len(actions.shape) == 1:\n",
        "        actions = np.expand_dims(actions, 1)\n",
        "    rewards = np.array(rewards)\n",
        "    if len(rewards.shape) == 1:\n",
        "        rewards = rewards.reshape(-1, 1)\n",
        "    dones = np.array(dones).reshape(-1, 1)\n",
        "\n",
        "    # Return in the following format\n",
        "    return dict(\n",
        "        observations=observations,\n",
        "        next_observations=next_observations,\n",
        "        actions=actions,\n",
        "        rewards=rewards,\n",
        "        dones=np.array(dones).reshape(-1, 1),\n",
        "        images=np.array(images)\n",
        "    )\n",
        "\n",
        "def mlp(input_dim, hidden_dim, output_dim, hidden_depth, output_mod=None):\n",
        "    if hidden_depth == 0:\n",
        "        mods = [nn.Linear(input_dim, output_dim)]\n",
        "    else:\n",
        "        mods = [nn.Linear(input_dim, hidden_dim), nn.ReLU(inplace=True)]\n",
        "        for i in range(hidden_depth - 1):\n",
        "            mods += [nn.Linear(hidden_dim, hidden_dim), nn.ReLU(inplace=True)]\n",
        "        mods.append(nn.Linear(hidden_dim, output_dim))\n",
        "    if output_mod is not None:\n",
        "        mods.append(output_mod)\n",
        "    trunk = nn.Sequential(*mods)\n",
        "    return trunk\n",
        "\n",
        "def rollout(\n",
        "        env,\n",
        "        agent,\n",
        "        episode_length=math.inf,\n",
        "        render=False,\n",
        "):\n",
        "    # Collect the following data\n",
        "    raw_obs = []\n",
        "    raw_next_obs = []\n",
        "    actions = []\n",
        "    rewards = []\n",
        "    dones = []\n",
        "    images = []\n",
        "\n",
        "    entropy = None\n",
        "    log_prob = None\n",
        "    agent_info = None\n",
        "    path_length = 0\n",
        "\n",
        "    o = env.reset()\n",
        "    if render:\n",
        "        env.render()\n",
        "\n",
        "    while path_length < episode_length:\n",
        "        o_for_agent = o\n",
        "\n",
        "        action, _, _ = agent(torch.Tensor(o_for_agent).unsqueeze(0).to(device))\n",
        "        action = action.cpu().detach().numpy()[0]\n",
        "\n",
        "        # Step the simulation forward\n",
        "        next_o, r, done, env_info = env.step(copy.deepcopy(action))\n",
        "\n",
        "        # Render the environment\n",
        "        if render:\n",
        "            env.render()\n",
        "\n",
        "        raw_obs.append(o)\n",
        "        raw_next_obs.append(next_o)\n",
        "        actions.append(action)\n",
        "        rewards.append(r)\n",
        "        dones.append(done)\n",
        "        path_length += 1\n",
        "        if done:\n",
        "            break\n",
        "        o = next_o\n",
        "\n",
        "    # Prepare the items to be returned\n",
        "    observations = np.array(raw_obs)\n",
        "    next_observations = np.array(raw_next_obs)\n",
        "    actions = np.array(actions)\n",
        "    if len(actions.shape) == 1:\n",
        "        actions = np.expand_dims(actions, 1)\n",
        "    rewards = np.array(rewards)\n",
        "    if len(rewards.shape) == 1:\n",
        "        rewards = rewards.reshape(-1, 1)\n",
        "    dones = np.array(dones).reshape(-1, 1)\n",
        "\n",
        "    # Return in the following format\n",
        "    return dict(\n",
        "        observations=observations,\n",
        "        next_observations=next_observations,\n",
        "        actions=actions,\n",
        "        rewards=rewards,\n",
        "        dones=np.array(dones).reshape(-1, 1),\n",
        "        images=np.array(images)\n",
        "    )\n",
        "\n"
      ],
      "metadata": {
        "id": "b7jfU0goMz0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(env, model, plan_mode, num_validation_runs=10, episode_length=200, render=False, mpc_horizon=None, n_samples_mpc=None, reward_fn=None):\n",
        "    success_count = 0\n",
        "    rewards_suc = 0\n",
        "    rewards_all = 0\n",
        "    for k in range(num_validation_runs):\n",
        "        o = env.reset()\n",
        "        path = collect_traj_MBRL(\n",
        "            env,\n",
        "            model,\n",
        "            plan_mode,\n",
        "            episode_length=episode_length,\n",
        "            render=render,\n",
        "            mpc_horizon=mpc_horizon,\n",
        "            n_samples_mpc=n_samples_mpc,\n",
        "            device=device,\n",
        "            reward_fn=reward_fn\n",
        "        )\n",
        "        success = np.linalg.norm(env.get_body_com(\"fingertip\") - env.get_body_com(\"target\"))<0.1\n",
        "\n",
        "        if success:\n",
        "            success_count += 1\n",
        "            rewards_suc += np.sum(path['rewards'])\n",
        "        rewards_all += np.sum(path['rewards'])\n",
        "        print(f\"test {k}, success {success}, reward {np.sum(path['rewards'])}\")\n",
        "    print(\"Success rate: \", success_count/num_validation_runs)\n",
        "    print(\"Average reward (success only): \", rewards_suc/max(success_count, 1))\n",
        "    print(\"Average reward (all): \", rewards_all/num_validation_runs)"
      ],
      "metadata": {
        "id": "lSTHO4qhN2U9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------- TODOs start from here --------------------------------------------------------------"
      ],
      "metadata": {
        "id": "sVGBLoy-N8g_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = torch.nn.MSELoss()\n",
        "\n",
        "def train_single(num_epochs, num_batches,batch_size, model, optimizer, replay_buffer):\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        for i in range(num_batches):\n",
        "            optimizer.zero_grad()\n",
        "            t1_observations, t1_actions, _, t1_next_observations, _ = replay_buffer.sample(batch_size)\n",
        "            oa_in = torch.cat([t1_observations, t1_actions], dim=-1)\n",
        "\n",
        "            next_o_pred = model(oa_in)\n",
        "            loss = loss_fn(next_o_pred, t1_next_observations)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "def train_model(model, replay_buffer, optimizer, num_epochs=500, batch_size=32):\n",
        "    \"\"\"\n",
        "    Train a single model with supervised learning\n",
        "    \"\"\"\n",
        "    idxs = np.array(range(len(replay_buffer)))\n",
        "    num_batches = len(idxs) // batch_size\n",
        "    if not isinstance(model, list):\n",
        "        train_single(num_epochs, num_batches, batch_size, model, optimizer, replay_buffer)\n",
        "\n",
        "    # TODO START-Ensemble models\n",
        "\n",
        "    # Hint1: try different batch size for each model\n",
        "    # hint2: check out how we define optimizer and model for ensemble models. During training, each model should have their individual optimizer and batch size to increase diversity.\n",
        "\n",
        "    # TODO END\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "M7Y7z6XXNdnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plan_model_random_shooting(env, state, ac_size, horizon, model, reward_fn, n_samples_mpc=100):\n",
        "    # TODO START-random MPC with shooting\n",
        "    # Hint1: randomly sample actions in the action space\n",
        "    # Hint2: rollout model based on current state and random action, select the best action that maximize the sum of the reward\n",
        "\n",
        "    # TODO END\n",
        "    return best_ac, random_actions[best_ac_idx]\n",
        "\n",
        "\n",
        "def plan_model_mppi(env, state, ac_size, horizon, model, reward_fn, n_samples_mpc=100, n_iter_mppi=10, gaussian_noise_scales=[1.0, 1.0, 0.5, 0.5, 0.2, 0.2, 0.1, 0.1, 0.01, 0.01]):\n",
        "    assert len(gaussian_noise_scales) == n_iter_mppi\n",
        "    # Rolling forward random actions through the model\n",
        "    state_repeats = torch.from_numpy(np.repeat(state[None], n_samples_mpc, axis=0)).cuda()\n",
        "    # Sampling random actions in the range of the action space\n",
        "    random_actions = torch.FloatTensor(n_samples_mpc, horizon, ac_size).uniform_(env.action_space.low[0], env.action_space.high[0]).cuda().float()\n",
        "    # Rolling forward through the mdoel for horizon steps\n",
        "    if not isinstance(model, list):\n",
        "        all_states, all_rewards = rollout_model(model, state_repeats, random_actions, horizon, reward_fn)\n",
        "    # TODO START-add ensemble MPPI\n",
        "    # Hint 1: rollout each model and concatenate rewards for each model\n",
        "\n",
        "\n",
        "    # TODO END\n",
        "\n",
        "\n",
        "\n",
        "    all_returns = all_rewards.sum(axis=-1)\n",
        "    # Take first action from best trajectory\n",
        "    # best_ac_idx = np.argmax(all_rewards.sum(axis=-1))\n",
        "    # best_ac = random_actions[best_ac_idx, 0] # Take the first action from the best trajectory\n",
        "\n",
        "    # Run through a few iterations of MPPI\n",
        "\n",
        "    # TODO START-MPPI\n",
        "    # Hint1: Compute weights based on exponential of returns\n",
        "    # Hint2: sample actions based on the weight, and compute average return over models\n",
        "    # Hint3: if model type is a list, then implement ensemble mppi\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # TODO END\n",
        "\n",
        "    # Finally take first action from best trajectory\n",
        "    best_ac_idx = np.argmax(all_rewards.sum(axis=-1))\n",
        "    best_ac = random_actions[best_ac_idx, 0] # Take the first action from the best trajectory\n",
        "    return best_ac, random_actions[best_ac_idx]\n",
        "\n",
        "\n",
        "def rollout_model(\n",
        "        model,\n",
        "        initial_states,\n",
        "        actions,\n",
        "        horizon,\n",
        "        reward_fn):\n",
        "    # Collect the following data\n",
        "    all_states = []\n",
        "    all_rewards = []\n",
        "    curr_state = initial_states # Starting from the initial state\n",
        "    # TODO START\n",
        "\n",
        "    # Hint1: concatenate current state and action pairs as the input for the model and predict the next observation\n",
        "    # Hint2: get the predicted reward using reward_fn()\n",
        "\n",
        "    # TODO END\n",
        "    all_states_full = torch.cat([state[:, None, :] for state in all_states], dim=1).cpu().detach().numpy()\n",
        "    all_rewards_full = torch.cat(all_rewards, dim=-1).cpu().detach().numpy()\n",
        "    return all_states_full, all_rewards_full\n",
        "\n",
        "def planning_agent(env, o_for_agent, model, reward_fn, plan_mode, mpc_horizon=None, n_samples_mpc=None):\n",
        "    if plan_mode == 'random':\n",
        "        # Taking random actions\n",
        "        action = torch.Tensor(env.action_space.sample()[None]).cuda()\n",
        "    elif plan_mode == 'random_mpc':\n",
        "        # Taking actions via random shooting + MPC\n",
        "        action, _ = plan_model_random_shooting(env, o_for_agent, env.action_space.shape[0], mpc_horizon, model,\n",
        "                                               reward_fn, n_samples_mpc=n_samples_mpc)\n",
        "    elif plan_mode == 'mppi':\n",
        "        action, _ = plan_model_mppi(env, o_for_agent, env.action_space.shape[0], mpc_horizon, model, reward_fn,\n",
        "                                    n_samples_mpc=n_samples_mpc)\n",
        "    else:\n",
        "        raise NotImplementedError(\"Other planning methods not implemented\")\n",
        "    return action\n",
        "\n",
        "def collect_traj_MBRL(\n",
        "        env,\n",
        "        model,\n",
        "        plan_mode,\n",
        "        replay_buffer=None,\n",
        "        device='cuda:0',\n",
        "        episode_length=math.inf,\n",
        "        reward_fn=None, #Reward function to evaluate\n",
        "        render=False,\n",
        "        mpc_horizon=None,\n",
        "        n_samples_mpc=None\n",
        "):\n",
        "    # Collect the following data\n",
        "    raw_obs = []\n",
        "    raw_next_obs = []\n",
        "    actions = []\n",
        "    rewards = []\n",
        "    dones = []\n",
        "    images = []\n",
        "\n",
        "    path_length = 0\n",
        "    o = env.reset()\n",
        "    if render:\n",
        "        env.render()\n",
        "\n",
        "    while path_length < episode_length:\n",
        "        o_for_agent = o\n",
        "\n",
        "        # Using the planning agent to take actions\n",
        "        action = planning_agent(env, o_for_agent, model, reward_fn, plan_mode, mpc_horizon=mpc_horizon, n_samples_mpc=n_samples_mpc)\n",
        "        action= action.cpu().detach().numpy()[0]\n",
        "\n",
        "        # Step the simulation forward\n",
        "        next_o, r, done, env_info = env.step(copy.deepcopy(action))\n",
        "        if replay_buffer is not None:\n",
        "            replay_buffer.add(o,\n",
        "                            action,\n",
        "                            r,\n",
        "                            next_o,\n",
        "                            done)\n",
        "        # Render the environment\n",
        "        if render:\n",
        "            env.render()\n",
        "\n",
        "        raw_obs.append(o)\n",
        "        raw_next_obs.append(next_o)\n",
        "        actions.append(action)\n",
        "        rewards.append(r)\n",
        "        dones.append(done)\n",
        "        path_length += 1\n",
        "        if done:\n",
        "            break\n",
        "        o = next_o\n",
        "\n",
        "    # Prepare the items to be returned\n",
        "    observations = np.array(raw_obs)\n",
        "    next_observations = np.array(raw_next_obs)\n",
        "    actions = np.array(actions)\n",
        "    if len(actions.shape) == 1:\n",
        "        actions = np.expand_dims(actions, 1)\n",
        "    rewards = np.array(rewards)\n",
        "    if len(rewards.shape) == 1:\n",
        "        rewards = rewards.reshape(-1, 1)\n",
        "    dones = np.array(dones).reshape(-1, 1)\n",
        "\n",
        "    # Return in the following format\n",
        "    return dict(\n",
        "        observations=observations,\n",
        "        next_observations=next_observations,\n",
        "        actions=actions,\n",
        "        rewards=rewards,\n",
        "        dones=np.array(dones).reshape(-1, 1),\n",
        "        images=np.array(images)\n",
        "    )\n",
        "\n",
        "# Training loop for policy gradient\n",
        "def simulate_mbrl(env, model, plan_mode, num_epochs=200, max_path_length=200, mpc_horizon=10, n_samples_mpc=200,\n",
        "                  batch_size=100, num_agent_train_epochs_per_iter=1000, capacity=100000, num_traj_per_iter=100, gamma=0.99, print_freq=10, device = \"cuda\", reward_fn=None):\n",
        "\n",
        "    # Set up optimizer and replay buffer\n",
        "    if not isinstance(model, list):\n",
        "        optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "    else:\n",
        "        print('Initialize separate optimizers for ensemble mbrl')\n",
        "        # TODO START\n",
        "        # Hint: try using separate optimizer with different learning rate for each model.\n",
        "        # TODO END\n",
        "    replay_buffer = ReplayBuffer(obs_size = env.observation_space.shape[0],\n",
        "                                 action_size = env.action_space.shape[0],\n",
        "                                 capacity=capacity,\n",
        "                                 device=device)\n",
        "\n",
        "    # Iterate through data collection and planning loop\n",
        "    for iter_num in range(num_epochs):\n",
        "        # Sampling trajectories\n",
        "        sample_trajs = []\n",
        "        if iter_num == 0:\n",
        "            # Seed with some initial data, collecting with mode random\n",
        "            for it in range(num_traj_per_iter):\n",
        "                sample_traj = collect_traj_MBRL(env=env,\n",
        "                                                model=model,\n",
        "                                                plan_mode='random',\n",
        "                                                replay_buffer=replay_buffer,\n",
        "                                                device=device,\n",
        "                                                episode_length=max_path_length,\n",
        "                                                reward_fn=reward_fn, #Reward function to evaluate\n",
        "                                                render=False,\n",
        "                                                mpc_horizon=None,\n",
        "                                                n_samples_mpc=None)\n",
        "                sample_trajs.append(sample_traj)\n",
        "        else:\n",
        "            for it in range(num_traj_per_iter):\n",
        "                sample_traj = collect_traj_MBRL(env=env,\n",
        "                                                model=model,\n",
        "                                                plan_mode=plan_mode,\n",
        "                                                replay_buffer=replay_buffer,\n",
        "                                                device=device,\n",
        "                                                episode_length=max_path_length,\n",
        "                                                reward_fn=reward_fn, #Reward function to evaluate\n",
        "                                                render=False,\n",
        "                                                mpc_horizon=mpc_horizon,\n",
        "                                                n_samples_mpc=n_samples_mpc)\n",
        "                sample_trajs.append(sample_traj)\n",
        "\n",
        "        # Train the model\n",
        "        train_model(model, replay_buffer, optimizer, num_epochs=num_agent_train_epochs_per_iter, batch_size=batch_size)\n",
        "\n",
        "        # Logging returns occasionally\n",
        "        if iter_num % print_freq == 0:\n",
        "\n",
        "            rewards_np = np.mean(np.asarray([traj['rewards'].sum() for traj in sample_trajs]))\n",
        "            path_length = np.max(np.asarray([traj['rewards'].shape[0] for traj in sample_trajs]))\n",
        "            print(\"Episode: {}, reward: {}, max path length: {}\".format(iter_num, rewards_np, path_length))"
      ],
      "metadata": {
        "id": "052S7bSaNfNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Args:\n",
        "    def __init__(self, model_type, plan_mode, test=False, render=False):\n",
        "        self.model_type = model_type\n",
        "        self.plan_mode = plan_mode\n",
        "        self.test = test # whether test only\n",
        "        self.render = render # whether to render during test\n",
        "args = Args('single', 'random_mpc', False,  False) # plan_mode choose from ['random_mpc', 'mppi'] and model_type choose from ['single', 'ensemble']"
      ],
      "metadata": {
        "id": "g1WTxnRUOqJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "random.seed(0)\n",
        "np.random.seed(0)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    if args.render:\n",
        "        import os\n",
        "        os.environ[\"LD_PRELOAD\"] = \"/usr/lib/x86_64-linux-gnu/libGLEW.so\"\n",
        "\n",
        "    # Environment and reward definition\n",
        "    env = gym.make(\"Reacher-v2\")\n",
        "    def reward_fn_reacher(state, action):\n",
        "        cos_theta = state[:, :2]\n",
        "        sin_theta = state[:, 2:4]\n",
        "        qpos = state[:, 4:6]\n",
        "        qvel = state[:, 6:8]\n",
        "        vec = state[:, 8:11]\n",
        "\n",
        "        reward_dist = -torch.norm(vec, dim=1)\n",
        "        reward_ctrl = -torch.square(action).sum(dim=1)\n",
        "\n",
        "        reward = reward_dist + reward_ctrl\n",
        "        reward = reward[:, None]\n",
        "        return reward\n",
        "    reward_fn = reward_fn_reacher\n",
        "\n",
        "    # Define dynamics model\n",
        "    hidden_dim_model = 64\n",
        "    hidden_depth_model = 2\n",
        "    if args.model_type == 'single':\n",
        "        model = DeterministicDynamicsModel(env.observation_space.shape[0] + env.action_space.shape[0], env.observation_space.shape[0], hidden_dim=hidden_dim_model, hidden_depth=hidden_depth_model)\n",
        "        model.to(device)\n",
        "    elif args.model_type == 'ensemble':\n",
        "        num_ensembles = 5\n",
        "        model = []\n",
        "        for model_id in range(num_ensembles):\n",
        "            curr_model = DeterministicDynamicsModel(env.observation_space.shape[0] + env.action_space.shape[0], env.observation_space.shape[0], hidden_dim=hidden_dim_model, hidden_depth=hidden_depth_model)\n",
        "            curr_model.to(device)\n",
        "            model.append(curr_model)\n",
        "    else:\n",
        "        raise NotImplementedError(\"No other model types implemented\")\n",
        "\n",
        "    # Training hyperparameters\n",
        "    num_epochs=15\n",
        "    max_path_length=50\n",
        "    batch_size=250 #5000\n",
        "    num_agent_train_epochs_per_iter=10 #100\n",
        "    num_traj_per_iter = batch_size // max_path_length\n",
        "    gamma=0.99\n",
        "    print_freq=1\n",
        "    capacity=100000\n",
        "    mpc_horizon = 10\n",
        "    n_samples_mpc = 1000\n",
        "\n",
        "    if not args.test:\n",
        "        # Training and model saving code\n",
        "        simulate_mbrl(env, model, plan_mode=args.plan_mode, num_epochs=num_epochs, max_path_length=max_path_length, mpc_horizon=mpc_horizon,\n",
        "                    n_samples_mpc=n_samples_mpc, batch_size=batch_size, num_agent_train_epochs_per_iter=num_agent_train_epochs_per_iter, capacity=capacity, num_traj_per_iter=num_traj_per_iter, gamma=gamma, print_freq=print_freq, device = \"cuda\", reward_fn=reward_fn)\n",
        "        if type(model) is list:\n",
        "            for model_idx, curr_model in enumerate(model):\n",
        "                torch.save(curr_model.state_dict(), f'{args.model_type}_{args.plan_mode}_{model_idx}.pth')\n",
        "        else:\n",
        "            torch.save(model.state_dict(), f'{args.model_type}_{args.plan_mode}.pth')\n",
        "    else:\n",
        "        print('loading pretrained mbrl')\n",
        "        if type(model) is list:\n",
        "            for model_idx in range(len(model)):\n",
        "\n",
        "                model[model_idx].load_state_dict(torch.load(f'{args.model_type}_{args.plan_mode}_{model_idx}.pth'))\n",
        "        else:\n",
        "            model.load_state_dict(torch.load(f'{args.model_type}_{args.plan_mode}.pth'))\n",
        "\n",
        "    evaluate(env, model, plan_mode=args.plan_mode, mpc_horizon=mpc_horizon, n_samples_mpc=n_samples_mpc, num_validation_runs=100, episode_length=max_path_length, render=args.render, reward_fn=reward_fn)"
      ],
      "metadata": {
        "id": "-nu62eUaPIMG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}